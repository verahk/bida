---
title: "results"
author: "Vera Kvisgaard"
date: "2024-08-20"
output: 
  html_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = T)
```

This note shows results from simulation experiments on BIDA with local structure.
Both in the structure learning part of the BIDA procedure and in the local-backdoor-estimation step, a partition of the relevant CPTs are learned in a
greedy structure: fitting either a tree-structure or a decision graph / labeled graph. 

## Parameter explantion
The different simulation settings are characterized by the following parameters:

- `network`: 
- `init `: algorithm for learning the initial search space for the MCMC chain. 
  Either`bnlearn::hc` or `pcalg::pc` is used to learn an CPDAG, and `*skel` means 
  that the undirected skeleton is used. The initial seach space is iteratively expanded with the `BiDAG:::IterativeMCMC` procedure.
- `struct`: Algorithm for optimizing the local CPTs. `none`for standard, full CPTs. `tree` or `ldag`.
- `sample`: MCMC scheme used to sample dags. `order`
- `ess`: equivalent sample size. Sat equal to 1. Defaults to .5 in BiDAG.
- `epf`: edge penalty. An penalty term `log(epf)*length(parentnodes)` is added to the score for each node. Defaults to 2 in BiDAG.
- `regular`: if `TRUE`, the partitioing over parent-outcomes is forced to be regular. I.e. to not encode any conditional independencies. Required for maximal-ldag-consistency.


```{r}
library(ggplot2)
here::i_am("inst/simulations/ldags/results.rmd")
indir <- here::here("inst/simulations/ldags/results/")
```



```{r}
# extract params from filenames
params_from_filenames <- function(files, 
                                  names = c("network", "init", "struct", "sample", "ess", "epf", "regular", "N", "r")) {
  tmp <- stringr::str_split(gsub("^(.*[\\/])|.rds", "", files), "_", simplify = FALSE)
  df <- data.frame(do.call(rbind, tmp), stringsAsFactors = F)
  names(df) <- names
  
  df$N <- factor(as.numeric(gsub("N", "", df$N)))
  
  levels <- unique(df$epf)
  levels <- levels[order(as.numeric(gsub("epf", "",  levels)))]
  df$epf <- factor(df$epf, levels)
  
  levels <- c("none", "tree", "ldag")
  df$struct <- factor(df$struct, levels)
  
  df$structr <- interaction(df$struct, df$regular)
  
  df
  
}

# prettify ggplots
prettify_plot <- function(plot) {
  list(theme_minimal(),
       theme(legend.position = "bottom"))
}

# plot box plot with ggplot
plot_boxplot <- function(df, x = "N", y, fill = "structr", 
                         facets = as.formula("init+sample+ess~network+epf")) {
  ggplot(df, aes(!! sym(x), !! sym(y), fill = !! sym(fill))) +
  facet_grid(facets) +
  geom_boxplot() +
  prettify_plot()
}
```

## Counts of each simulation setting
For each simulation setting, up to 30 simulation runs. When the table below shows
a lower frequency for some settings, the simulation is not complete.
At each run, the different structure learning procedures are applied to a single
data set sampled from the network. 
```{r}
files <- list.files(indir, ".rds", full.names = TRUE)
df_par <- params_from_filenames(files)
include <- !names(df_par) %in% c("N" , "r", "structr")
data.frame(table(df_par[, include]))
```


# Asia, 8 nodes
The Asia network is the smallest and easiest to run with a wide range of different simulation settings, shown in the following. 

```{r}

# import simulation results
files <- list.files(indir, pattern = "asia.*.rds", full.names = T)
imp <- lapply(files, readRDS)
tmp <- lapply(imp, "[[", "res")
len <- lengths(tmp)
stopifnot(all(len == len[1]))

# store in data frame
df <- cbind(params_from_filenames(files), do.call(rbind, tmp))
```

## True and false positive rate 
The true and false positive rate of the edges (TPR and FPR, respectively), averaged over all DAGs in the sample. The higher FPR when learning DAGs with local structure suggests spurious edges. Mitigated by increasing the edge penalty parameter `epf`.
```{r}
plot_boxplot(df, "N", "tpr")
plot_boxplot(df, "N", "fpr")
```


## Precision-recall 
The area under the precision recall curve, approximated by the average precision (`avgppv`). Based on the posterior edge probabilities (`edgep`) and the posterior 
ancestor relation probabilities (`arp`).
```{r}
plot_boxplot(df, "N", "edgep")
plot_boxplot(df, "N", "arp")
```

## Intervention distribution
Mean squared error of the point-estimates intervention distributions.
Tend to be more accurate with `tree`. Might sug
```{r}
plot_boxplot(df, "N", "mse")
```


# Larger networks
```{r}
files <- list.files(indir, pattern = ".rds", full.names = T)
files <- files[!grepl("asia", files)]

if (length(files) > 0) {
  imp <- lapply(files, readRDS)
  df <- cbind(params_from_filenames(files), do.call(rbind, tmp))
  
  # true positive edge rate
  plot_boxplot(df, "N", "tpr")
  
  # false positive edge rate 
  plot_boxplot(df, "N", "fpr")
  
  # average precision, edges
  plot_boxplot(df, "N", "edgep")
  
  # average precision, arp
  plot_boxplot(df, "N", "arp")
  
  # mse of intervention probs
  plot_boxplot(df, "N", "mse")
}

```


