---
title: "Sim_slearn_with_lstruct"
subtitle: "Simulation: BIDA on random DAGs with tree-CPTs.
author: "Vera Kvisgaard"
date: "2024-09-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The ground truth networks are generated by first sampling a random DAG of size $n$ with expected indegree $d$. 
The local structure of the network is then drawn by growing a random decision tree partitioning the parent space of each node (with more than 1 parent). 
The depth of these trees controls the CSI-complexity. 
The depth of the trees are determined by a factor $rr$ which determines the number of splits in each branch of the tree relative to the number of parents. Here, trees are grown with $rr = .25$ (few params, high complexity) or $rr = .75$ (many params, low complexity). 
Importantly, the partition might be encoding conditional independencies such that some child-parent pairs are marginally independent. 
When spurious edges is taken to be part of the ground truth, e.g. the ancestor relation probs would be misleading.
**Should make the ground-truth-DAG consistent with the global+local structure - but then harder to describe the underlying DAG with expected indegree param.** 

The iterative MCMC scheme implemented in the BiDAG-package is used to sample DAGs, where the score of each node is given by the reduced CPT learn by optimizing the local structure. 
As of know, only the `ptree` rountine is implemented. 

