% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimize_partition_from_data.R
\name{optimize_partition_from_data_tree}
\alias{optimize_partition_from_data_tree}
\alias{predict.tree}
\title{Greedy optimization of local structure: decision trees}
\usage{
optimize_partition_from_data_tree(
  data,
  ess,
  nlev,
  min_improv,
  prune = FALSE,
  regular = FALSE,
  verbose = FALSE
)

\method{predict}{tree}(fit, newdata, name = "part")
}
\arguments{
\item{data}{(numeric matrix) a matrix with integers representing zero-based
categorical cariables. The response variable is assumed placed in the first
column, the predictors in the remaining.}

\item{ess}{()}

\item{min_improv}{(numeric) minimum score improvement for the next split.
If 0, splits are added greedily until no split improves the score.
If \code{-Inf}, full trees are grown, choosing at each iteration
the split that gives the least worsening of the score.}

\item{prune}{(bolean) if \code{TRUE} the tree is pruned, that is all subtrees that
do not improve the score is removed. Defaults to \code{FALSE}.}

\item{regular}{(bolean) if \code{TRUE}, the partition is forced regular by adding
to each leaf a split for each variable that is not present in the fitted tree.
Defaults to \code{FALSE}.}

\item{verbose}{(bolean) if \code{TRUE}, messages is printed during the fitting process.}
}
\value{
an object of class \code{partition} with sub-class \code{tree}
}
\description{
This function learn a partition of the parent space, using a greedy recursive
decision tree algorithm. At every non-leaf node, one branch is added to the tree
for every level of the split variable. The score of the tree is the sum of its
leaf-scores. The next split is found by iterating through every variable
that is yet not a split variable in the current branch and compute the score
of the new leaves.
}
\seealso{
\code{\link[=optimize_partition_from_data]{optimize_partition_from_data()}}
}
\keyword{internal}
